{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "X = iris.loc[:, iris.columns != \"label\"].values\n",
    "\n",
    "y = iris[\"label\"].values\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'Iris-setosa':\n",
    "        y[i] = np.array([1, 0, 0])\n",
    "    elif y[i] == 'Iris-versicolor':\n",
    "        y[i] = np.array([0, 1, 0])\n",
    "    else:\n",
    "        y[i] = np.array([0, 0, 1])\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, train_x, train_y, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weightsl1 = np.random.randn(input_nodes, hidden_nodes)\n",
    "        self.weightsl2 = np.random.randn(hidden_nodes, output_nodes)\n",
    "        self.biasl1 = np.random.randn(hidden_nodes)\n",
    "        self.biasl2 = np.random.randn(output_nodes)\n",
    "        \n",
    "    def relu(self, x, deriv=False):\n",
    "        if deriv: return 1 * (x > 0)\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def softmax(self, A):\n",
    "        expA = np.exp(A)\n",
    "        return expA / expA.sum()\n",
    "\n",
    "    def cross_entropy(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        p = self.softmax(X)\n",
    "        log_likelihood = -np.log(p[range(m),y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def feedforward(self, input):\n",
    "        hidden = self.relu(np.dot(input, self.weightsl1) + self.biasl1)\n",
    "        z = self.softmax(np.dot(hidden, self.weightsl2) + self.biasl2)\n",
    "        return z\n",
    "\n",
    "    def train(self, iterations=10000):\n",
    "        loss = 0\n",
    "        for i in range(iterations):\n",
    "            ########## Feedforward\n",
    "\n",
    "            ri = np.random.randint(len(self.train_x)) # Random datapoint\n",
    "            layer_0 = self.train_x[ri:ri+1]\n",
    "            layer_1 = self.relu(np.dot(layer_0, self.weightsl1) + self.biasl1)\n",
    "            layer_2 = self.softmax(np.dot(layer_1, self.weightsl2) + self.biasl2)\n",
    "\n",
    "            ########## Back Propagation\n",
    "\n",
    "            loss += self.cross_entropy(layer_2, np.array([self.train_y[ri]])) # Define the loss function\n",
    "\n",
    "            layer_2_delta = layer_2 - self.train_y[ri] # Derivative of the cross entropy loss function\n",
    "\n",
    "            dcost_wl2 = layer_1.T.dot(layer_2_delta) # layer 2 weights change\n",
    "            dcost_bl2 = layer_2_delta # Layer 2 bias change\n",
    "            \n",
    "            layer_1_delta = layer_2_delta.dot(self.weightsl2.T) * self.relu(layer_1, deriv=True) # change with respect to the first layer\n",
    "\n",
    "            dcost_wl1 = layer_0.T.dot(layer_1_delta) # layer 1 weights change\n",
    "            dcost_bl1 = layer_1_delta # layer 1 bias change\n",
    "\n",
    "            ########## Update weights using gradient descent\n",
    "\n",
    "            self.weightsl1 -= self.learning_rate * dcost_wl1\n",
    "            self.biasl1 -= self.learning_rate * dcost_bl1.sum(axis=0)\n",
    "\n",
    "            self.weightsl2 -= self.learning_rate * dcost_wl2\n",
    "            self.biasl2 -= self.learning_rate * dcost_bl2.sum(axis=0)\n",
    "\n",
    "            ########## print average loss\n",
    "\n",
    "            if (i-999) % 1000 == 0:\n",
    "                print(\"Iteration: {}, loss: {}\".format(i+1, loss/i))\n",
    "                loss = 0\n",
    "\n",
    "    def check_output(self, test_x, test_y):\n",
    "        correct = 0\n",
    "        for i in range(len(test_x)):\n",
    "            point = test_x[i]\n",
    "            target = list(test_y[i])\n",
    "            z = list(self.feedforward(point))\n",
    "            prediction_index = z.index(max(z))\n",
    "            target_index = target.index(max(target))\n",
    "            if prediction_index == target_index: correct += 1\n",
    "            print(z, target)\n",
    "        print(\"Test Accuracy: {}\".format((correct / len(test_x)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NeuralNetwork(4, 5, 3, train_X, train_y, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, loss: 3.622456126086912\n",
      "Iteration: 2000, loss: 1.7762067114326412\n",
      "Iteration: 3000, loss: 1.1771434104453187\n",
      "Iteration: 4000, loss: 0.8861488698147758\n",
      "Iteration: 5000, loss: 0.7130297548483933\n",
      "Iteration: 6000, loss: 0.5977097303098513\n",
      "Iteration: 7000, loss: 0.5177761852750062\n",
      "Iteration: 8000, loss: 0.4550965589974584\n",
      "Iteration: 9000, loss: 0.40052487065864645\n",
      "Iteration: 10000, loss: 0.366717621760664\n"
     ]
    }
   ],
   "source": [
    "n.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7965026528682994e-09, 0.007891633209732832, 0.9921083649937645] [0, 0, 1]\n",
      "[0.00010798324549016774, 0.9842105057714257, 0.015681510983084263] [0, 1, 0]\n",
      "[0.9957615368477596, 0.004238463107079104, 4.516130538959267e-11] [1, 0, 0]\n",
      "[8.186945048404458e-12, 0.004346744111761302, 0.9956532558800517] [0, 0, 1]\n",
      "[0.9932988115682815, 0.0067011881085231005, 3.231954630520379e-10] [1, 0, 0]\n",
      "[1.1634409577517125e-11, 0.0008481521118028136, 0.9991518478765629] [0, 0, 1]\n",
      "[0.9933103305697687, 0.006689669109418423, 3.208130240905355e-10] [1, 0, 0]\n",
      "[9.102794069856454e-05, 0.9834794654933662, 0.01642950656593519] [0, 1, 0]\n",
      "[1.3591250492376239e-05, 0.9473513680793284, 0.05263504067017924] [0, 1, 0]\n",
      "[0.0013792063296849972, 0.9959682514654562, 0.002652542204858828] [0, 1, 0]\n",
      "[2.3004638897800847e-10, 0.008412607262520093, 0.9915873925074336] [0, 0, 1]\n",
      "[0.0004195448052721252, 0.991476055961072, 0.008104399233655806] [0, 1, 0]\n",
      "[2.114617115752166e-05, 0.8832958743049445, 0.116682979523898] [0, 1, 0]\n",
      "[3.186343362310307e-05, 0.9482750962216118, 0.051693040344765] [0, 1, 0]\n",
      "[2.4092208695835907e-05, 0.861330156454086, 0.1386457513372181] [0, 1, 0]\n",
      "[0.9932400086206162, 0.006759991043813572, 3.3557020109162847e-10] [1, 0, 0]\n",
      "[6.8185228673425e-05, 0.9193735956007267, 0.08055821917059974] [0, 1, 0]\n",
      "[2.9102068022237416e-05, 0.7792979128683117, 0.22067298506366603] [0, 1, 0]\n",
      "[0.9919235915427732, 0.008076407735657496, 7.21569256665993e-10] [1, 0, 0]\n",
      "[0.9950259500618068, 0.0049740498484130225, 8.977999901438443e-11] [1, 0, 0]\n",
      "[6.141743308815213e-08, 0.046024363779113495, 0.9539755748034534] [0, 0, 1]\n",
      "[7.99084258323641e-05, 0.8394189826314592, 0.16050110894270847] [0, 1, 0]\n",
      "[0.9905461426754979, 0.009453855902617964, 1.4218841564755062e-09] [1, 0, 0]\n",
      "[0.9888480248892312, 0.011151972211716436, 2.8990523395247425e-09] [1, 0, 0]\n",
      "[2.4088233717724812e-06, 0.48294168927751047, 0.5170559018991179] [0, 0, 1]\n",
      "[0.9906728570864212, 0.009327142272214818, 6.413640200826904e-10] [1, 0, 0]\n",
      "[0.9917348227013735, 0.008265176501589208, 7.970374236706256e-10] [1, 0, 0]\n",
      "[0.0003924163428755699, 0.9916079654050582, 0.007999618252066212] [0, 1, 0]\n",
      "[0.006597812786576607, 0.9903841174108967, 0.0030180698025268076] [0, 1, 0]\n",
      "[0.9921222051173718, 0.007877794234400987, 6.482272756891587e-10] [1, 0, 0]\n",
      "[2.811576205565742e-08, 0.07676793057850652, 0.9232320413057314] [0, 0, 1]\n",
      "[6.253204770844663e-05, 0.7436528964475421, 0.2562845715047496] [0, 1, 0]\n",
      "[0.9944506334121676, 0.00554936644416018, 1.4367217258497602e-10] [1, 0, 0]\n",
      "[3.177678221903195e-06, 0.4582990145477797, 0.5416978077739985] [0, 0, 1]\n",
      "[9.489489294078484e-11, 0.0041837613522757115, 0.9958162385528293] [0, 0, 1]\n",
      "[0.000712574144395164, 0.9594541571290566, 0.03983326872654831] [0, 1, 0]\n",
      "[0.9960154769765533, 0.003984522988804643, 3.464202500833758e-11] [1, 0, 0]\n",
      "[5.814894590668269e-08, 0.09444742793342964, 0.9055525139176245] [0, 1, 0]\n",
      "Test Accuracy: 97.36842105263158\n"
     ]
    }
   ],
   "source": [
    "n.check_output(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
